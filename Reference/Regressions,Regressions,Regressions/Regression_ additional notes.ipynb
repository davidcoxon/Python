{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression - Additional Notes.\n",
    "We created a notebook called Regression, regression, regression  comparing different regression techniques, that notebook got a bit long and a bit off topic, this notebook contains some of the additional material taken out of that document to keep it focussed.\n",
    "\n",
    "In section 2 we discovered a relationship between child_mortality and fertility, we go on to  plot this here to see that it is infact correct.\n",
    "\n",
    "## Plotting a linear regression for child mortality and fertility\n",
    "\n",
    "Having learnt from our Lasso regression that child mortality has a stronger relationship to fertility that life expectancy, it is worth plotting the linear regression for child-martality and fertility to see how close this relationship is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit & predict - Linear Regression ##\n",
    "\n",
    "# Import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def pearson_r(x, y):\n",
    "    \"\"\"Compute Pearson correlation coefficient between two arrays.\"\"\"\n",
    "    # Compute correlation matrix: corr_mat\n",
    "    corr_mat=np.corrcoef(x,y)\n",
    "    # Return entry [0,1]\n",
    "    return corr_mat[0,1]\n",
    "\n",
    "# Create arrays for features and target variable\n",
    "y =lf['fertility'].values\n",
    "X_mortality=lf['child_mortality'].values\n",
    "\n",
    "print(\"Pearson's Coefficient : \" + (str(pearson_r(y,X_mortality))))\n",
    "\n",
    "# Reshape X and y\n",
    "y = y.reshape(-1,1)\n",
    "X_mortality = X_mortality.reshape(-1,1)\n",
    "\n",
    "# Create the regressor: reg\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Create the prediction space\n",
    "prediction_space = np.linspace(min(X_mortality), max(X_mortality)).reshape(-1,1)\n",
    "\n",
    "# Fit the model to the data\n",
    "reg.fit(X_mortality, y)\n",
    "\n",
    "# Compute predictions over the prediction space: y_pred\n",
    "y_pred = reg.predict(prediction_space)\n",
    "\n",
    "# Print R^2 \n",
    "print(\"Score :\" + (str(reg.score(X_mortality,y))))\n",
    "\n",
    "# Plot data points\n",
    "plt.scatter(X_mortality, y,marker='.',color='green')\n",
    "\n",
    "# Plot regression line\n",
    "plt.plot(prediction_space, y_pred, color='purple', linewidth=2)\n",
    "\n",
    "# Label Graph\n",
    "_ = plt.title('Supervised Learning - Child mortality and Fertility')\n",
    "_ = plt.xlabel('Child Mortality')\n",
    "_ = plt.ylabel('Fertility')\n",
    "\n",
    "#show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pearson's coefficient for Life Expectancy and fertility was 0.78 but child mortality and fertility is higher at 0.91 which suggest that there is stronger relationship between child mortality and fertility than there is between life expectancy and fertility.\n",
    "\n",
    "So while there was little difference between the Linear regression plots generated by Least squares and supervised learning for the life expectancy and fertility relationhip, the supervised learning lasso regression produced a far more insightful result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction : Unsupervised Learning\n",
    "\n",
    "While Linear Regression relates to supervised rather than unsepvised learning, you can use unsupervised learnng to identify principal components and to reduce dimensions.\n",
    "\n",
    "## Dimension Reduction\n",
    "\n",
    "Dimension reduction seeks to simplify data by looking for patterns in the data, and then using these pattens to re-express the data in a compressed form. This makes subsequent expression of the data much more efficient, which can be very useful for larger datasets/big data.\n",
    "\n",
    "Dimension reduction also seeks remove less-informative \"noise\" features, which cause problems for prediction tasks, e.g. classification, regressions.\n",
    "\n",
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "The most fundamental dimension reduction technique is Principal Component Analysis.\n",
    "\n",
    "Principal Component Analysis consists of 2 steps, \"decorrelation\" which doesn't change the dimentions of the data at all the second step is diminsion reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Principal Component Analysis (PCA) ##\n",
    "\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import data\n",
    "# import the data\n",
    "lf=pd.read_csv('Data/gm_2008_region.csv',encoding = \"ISO-8859-1\", sep=',',index_col='Unnamed: 0')\n",
    "\n",
    "# drop column\n",
    "samples= lf.drop('Region', axis=1)\n",
    "samples= samples.drop(['population',\"GDP\"], axis=1)\n",
    "\n",
    "# create labels\n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=4)\n",
    "model.fit(samples)\n",
    "labels = model.predict(samples)\n",
    "\n",
    "# plot original dataset\n",
    "plt.figure(1)\n",
    "plt.scatter((samples['fertility']),(samples['life']),c=labels)\n",
    "\n",
    "# Set the x-axis range\n",
    "plt.ylim(45,85)\n",
    "\n",
    "plt.show\n",
    "\n",
    "#import library\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# instanciate PCA model and .fit() data to it\n",
    "model = PCA()\n",
    "model.fit(samples)\n",
    "\n",
    "# .transform() the data\n",
    "transformed = model.transform(samples)\n",
    "\n",
    "# plot transformed dataset\n",
    "transformeddf=pd.DataFrame(data=transformed[0:,0:])\n",
    "plt.figure(2)\n",
    "plt.scatter(transformeddf[0],transformeddf[1],c=labels)\n",
    "plt.axhline(0, color='black')\n",
    "plt.axvline(0, color='black')\n",
    "plt.show\n",
    "\n",
    "# Calculate the Pearson correlation of xs and ys\n",
    "correlation, pvalue = pearsonr((samples['fertility']),(samples['life']))\n",
    "\n",
    "# Display the correlation\n",
    "print(\"Pearson's correlation : \" + str(correlation))\n",
    "\n",
    "# Calculate the Pearson correlation of xs and ys\n",
    "correlation, pvalue = pearsonr(transformeddf[0],transformeddf[1])\n",
    "\n",
    "# Display the correlation\n",
    "print(\"Pearson's correlation for Principal Components : \" + str(correlation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the variances of PCA featuresÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computer the intrincic dimensions\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# Create scaler: scaler\n",
    "scaler = StandardScaler()\n",
    "# Create a PCA instance: pca\n",
    "pca = PCA()\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler,pca)\n",
    "# Fit the pipeline to 'samples'\n",
    "pipeline.fit(samples)\n",
    "# Plot the explained variances\n",
    "plt.figure(1)\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features,pca.explained_variance_)\n",
    "plt.xlabel('PCA feature')\n",
    "plt.ylabel('variance')\n",
    "plt.xticks(features)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## The boston house price linear regression example with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## multi feature Linear regression with Tensorflow and Keras ##\n",
    "## Sample from BOSTON house price Sample ##\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"Data/boston.csv\", sep=',',index_col='Unnamed: 0')\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:13]\n",
    "Y = dataset[:,13]\n",
    "\n",
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
